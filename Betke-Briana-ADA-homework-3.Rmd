---
title: "Betke-Briana-ADA-homework-3"
author: "Briana Betke"
date: "4/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework 3
Let's start with reading in the appropriate packages for the assignment:
```{r  message=FALSE}
library(tidyverse)
library(broom)
library(cowplot)
theme_set(theme_cowplot())
```

An then read in the Kamilar and Cooper dataset from online:
```{r message=FALSE}
# reading data
f <- "https://raw.githubusercontent.com/difiore/ADA-datasets/master/KamilarAndCooperData.csv"
d <- read_csv(f, col_names = TRUE)
head(d,3) # just the first 6 so its not so much when knitted.
```

For challenge 1, I wanted to add the log transformed variables to the dataset.
```{r}
d <- d %>%
  mutate(log_Maxlongevity_m = log(MaxLongevity_m), # mutate to add colmuns to the dataset
         log_Brain_Size_Species_Mean = log(Brain_Size_Species_Mean))
colnames(d) # just to look at the variable names
```


### Challenge 1  
#### Fit the regression models
Starting with untransformed:
```{r}
mod <- lm(MaxLongevity_m~Brain_Size_Species_Mean, data = d)
tidy(mod) # model output, but nice
```

Now the transformed one:
```{r}
log_mod <- lm(log_Maxlongevity_m~log_Brain_Size_Species_Mean, data = d)
tidy(log_mod)
```

#### Superimpose line on ggplot and add model equation to the plot
First, I wanted to take out the coefficients of the models to embed into the equation for the graph. So I save the tidy model output as a data frame then put them into the paste function to test how it would look. This took a while because I am bad with paste functions.....apparently. 
```{r}
values <- as.data.frame(tidy(mod)) # the data frame

# testing the paste
paste("longevity = ", paste(round(values$estimate[1],2), paste(round(values$estimate[2],2), "brain size", sep = "*"), sep=" + "), sep="")
```

transformed version
```{r}
log_values <- as.data.frame(tidy(log_mod))

paste("longevity = ", paste(round(log_values$estimate[1],2), paste(round(log_values$estimate[2],2), "brain size", sep = "*"), sep=" + "), sep="")
```

Now on to ggplot! Adding the equation to the graph using the geom_text().
```{r message=FALSE, warning=FALSE}
mod_plot <- ggplot(d, aes(y = MaxLongevity_m, x = Brain_Size_Species_Mean)) +
        geom_point()+
        geom_smooth(method = "lm", color = "black", formula = y ~ x) + 
        geom_text(x = 220, y = 950, size = 3, label = paste("longevity = ",
                                                  paste(round(values$estimate[1],2),
                                                        paste(round(values$estimate[2],2),
                                                              "brain size", sep = "*"), sep=" + "), sep=""))

log_mod_plot <- ggplot(d, aes(y = log(MaxLongevity_m), x = log(Brain_Size_Species_Mean))) +
        geom_point()+
        geom_smooth(method = "lm", color = "black", formula = y ~ x) + 
        geom_text(x = 3.0, y = 6.60, size = 3, label = paste("longevity = ",
                                                  paste(round(log_values$estimate[1],2),
                                                        paste(round(log_values$estimate[2],2),
                                                              "brain size", sep = "*"), sep=" + "), sep=""))
# multiplot time 
library(patchwork)
mod_plot + log_mod_plot
```

#### Hypothesis test

H0: There is no linear relationship (β1=0)  
HA: There is a linear relationship (β1≠0)

The p-values for the explanatory variable in both models are less than 0.05.Indicating that there is significant evidence of a linear relationship.

Transformed:  
Viewing the model again, the estimate is 1.21. This can be interpreted as the increase in longevity (in months) for every 1 gm increase in mean brain size is 1.21, on average. 

With the transformations:  
For every 1% increase in brain size, longevity increases 23%, on average. 

```{r}
values
log_values
```

90% CI for slope estimate untransformed:
```{r}
alpha <- 0.10
(CI <- tidy(mod, conf.int = TRUE, conf.level = 1 - alpha))
```

90% CI for slope estimate transformed:
```{r}
(CI <- tidy(log_mod, conf.int = TRUE, conf.level = 1 - alpha))
```

#### Adding 90% CI and Prediction interval to scatter plot
First with the untransformed variables: 
```{r warning=FALSE}
ci <- predict(mod,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "confidence", level = 1 - alpha
)
ci <- data.frame(ci)
ci <- cbind(d$Brain_Size_Species_Mean,ci)
names(ci) <- c("Brain_Size_Species_Mean", "c.fit", "c.lwr", "c.upr")

# prediction interval 
pi <- predict(mod,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "prediction", level = 0.90
) 

pi <- data.frame(pi)
pi <- cbind(d$Brain_Size_Species_Mean,pi)
names(pi) <- c("Brain_Size_Species_Mean", "p.fit", "p.lwr", "p.upr")

mod_plot2 <- ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m)) +
  geom_point() +
  geom_line(data = ci, aes(x = Brain_Size_Species_Mean, y = c.fit, color = "Fit")) +
  geom_line(data = ci, aes(x = Brain_Size_Species_Mean, y = c.lwr, color = "CI")) +
  geom_line(data = ci, aes(x = Brain_Size_Species_Mean, y = c.upr, color = "CI")) +
  geom_line(data = pi, aes(x = Brain_Size_Species_Mean, y = p.lwr, color = "PI")) +
  geom_line(data = pi, aes(x = Brain_Size_Species_Mean, y = p.upr, color = "PI"))

(mod_plot_2 <- mod_plot2 + scale_color_manual(name = "lines",
                        breaks = c("Fit", "CI", "PI"),
                        values = c("Fit" = "black", "CI" = "blue", "PI" = "red")))
```

The untransformed variable:
```{r warning=FALSE}
log_ci <- predict(log_mod,
  newdata = data.frame(log_Brain_Size_Species_Mean = d$log_Brain_Size_Species_Mean),
  interval = "confidence", level = 1 - alpha
)
log_ci <- data.frame(log_ci)
log_ci <- cbind(log(d$Brain_Size_Species_Mean),log_ci)
names(log_ci) <- c("Brain_Size_Species_Mean", "c.fit", "c.lwr", "c.upr")

# prediction interval 
log_pi <- predict(log_mod,
  newdata = data.frame(log_Brain_Size_Species_Mean = d$log_Brain_Size_Species_Mean),
  interval = "prediction", level = 0.90
) 

log_pi <- data.frame(log_pi)
log_pi <- cbind(log(d$Brain_Size_Species_Mean),log_pi)
names(log_pi) <- c("Brain_Size_Species_Mean", "p.fit", "p.lwr", "p.upr")

log_mod_plot2 <- ggplot(data = d, aes(x = log_Brain_Size_Species_Mean, y = log_Maxlongevity_m)) +
  geom_point() +
  geom_line(data = log_ci, aes(x = Brain_Size_Species_Mean, y = c.fit, color = "Fit")) +
  geom_line(data = log_ci, aes(x = Brain_Size_Species_Mean, y = c.lwr, color = "CI")) +
  geom_line(data = log_ci, aes(x = Brain_Size_Species_Mean, y = c.upr, color = "CI")) +
  geom_line(data = log_pi, aes(x = Brain_Size_Species_Mean, y = p.lwr, color = "PI")) +
  geom_line(data = log_pi, aes(x = Brain_Size_Species_Mean, y = p.upr, color = "PI"))
(log_mod_pot2 <- log_mod_plot2 + scale_color_manual(name = "lines",
                        breaks = c("Fit", "CI", "PI"),
                        values = c("Fit" = "black", "CI" = "blue", "PI" = "red")))
```

#### Point estimate
Untransformed model
```{r}
pi_single <- predict(mod,
  newdata = data.frame(Brain_Size_Species_Mean = 750),
  interval = "prediction", level = 0.90
)
pi_single
```

Transformed model:
```{r}
log_pi_single <- predict(log_mod,
  newdata = data.frame(log_Brain_Size_Species_Mean = log(750)),
  interval = "prediction", level = 0.90
)
log_pi_single
```

I would not trust the models to predict the logevity of this value accurately. Looking at the maximum value of brain size for the variable, 750 gm is not a value contained in the dataset. The highest value in the dataset is 491.27 gm. Using this value in the model would be considered extrapolation as this is applying a value outside of the range of the observed data in which the model was generated.

#### Looking @ the models, which is better and why?
First, lets look at the residuals. For the untransformed data 
```{r}
qqnorm(mod$residuals)
qqline(mod$residuals, col = "red")

plot(mod$fitted.values, mod$residuals, xlab = "Fitted Values", 
    ylab = "Residuals", main = "Residual Plot", pch = 20)
abline(h = 0, col = "red")
```

```{r}
qqnorm(log_mod$residuals)
qqline(log_mod$residuals, col = "red")

plot(log_mod$fitted.values, log_mod$residuals, xlab = "Fitted Values", 
    ylab = "Residuals", main = "Residual Plot", pch = 20)
abline(h = 0, col = "red")
```

I would say that the model with the log transformed variables is better. The transformations seem to improve normality and equal varience of the residuals. 

### Challenge 2:

Run the lm:
```{r}
tidy(lm(log(HomeRange_km2)~log(Body_mass_female_mean), data = d))
```

On to the boostrapping. I get now that I didn't have to go the list route...but I did...
```{r message=FALSE}
# selecting columns for analysis for simplicity. Removing NAs because it seemed to solve my loop problems...?
r <- d %>%
  select(HomeRange_km2, Body_mass_female_mean) %>%
  drop_na()

# Now set up the loop
k <- 1000
n <- dim(r)[1]

s <- list(length = k)
coef <- list(length = k)

for (i in 1:k) {
  s[[i]] <- sample_n(r, size = n, replace = TRUE)
  m <- s[[i]]
  model <- lm(log(HomeRange_km2) ~ log(Body_mass_female_mean), data = m)
  coef[[i]] <- model$coefficients
}

boot_coef <- data.frame(matrix(unlist(coef), nrow=k, byrow=T),stringsAsFactors=FALSE)
names(boot_coef) <- c("boot_int","boot_slope")

hist(boot_coef$boot_int)
hist(boot_coef$boot_slope)
```

#### standard error
```{r}
sd(boot_coef$boot_int)
sd(boot_coef$boot_slope)
```
#### CI
For the intercept
```{r}
alpha <- 0.05
int_CI <- mean(boot_coef$boot_int) + c(-1, 1) * qt(1 - alpha / 2, df = n - 1) * sd(boot_coef$boot_int)
names(int_CI) <- c("lower","upper")
int_CI
```
For the slope
```{r}
slope_CI <- mean(boot_coef$boot_slope) + c(-1, 1) * qt(1 - alpha / 2, df = n - 1) * sd(boot_coef$boot_slope)
names(slope_CI) <- c("lower","upper")
slope_CI
```

#### how do they compare to the model?
Model confidence intervals
```{r}
model <- lm(log(HomeRange_km2)~log(Body_mass_female_mean), data = d) 
tidy(model)
confint(model)
```

The standard errors and CIs for both coeffecients from the bootstrap are fairly close close to that of the single linear model. 

### Challenge 3
#### Write a function
It took so long to make to boot part of the the boot_lm. So I didn't have time to combine my two functions...The first one gives the estimates, SE, and CI of the model alone and the second function is the boot function that only runs the bootstrapping and resturns results in a similar format but without the slope name....  
So perhaps there was no point in makeing the first function. I intended to put it in the boot_lm and combine the tables.

```{r}
lm_sum <-  function(d, model = "", conf.level = ""){
  mod <- lm(model, d)
  CI <- tidy(mod, conf.int = TRUE, conf.level = conf.level)
  CI %>% select(term, estimate, std.error, conf.low, conf.high)
}

boot_lm <- function(d, model = "", reps = "", conf.level = ""){
  
  # Setting up the boot strap
  n <- dim(d)[1]
  s <- list(length = reps)
  coef <- list(length = reps)
  
  for (i in 1:reps) {
    s[[i]] <- sample_n(d, size = n, replace = TRUE)
    m <- s[[i]]
    model <- lm(model, data = m) 
    coef[[i]] <- model$coefficients
  }
  
  # Turn the coefficients into a data frame
  boot_Mat <- data.frame(matrix(unlist(coef), nrow=reps, byrow=T),stringsAsFactors=FALSE)
  names(boot_Mat) <- c("boot_int","boot_slope")
  
  # Get the se for the slope and intercept
  boot_slope_se <- sd(boot_Mat$boot_slope)
  boot_int_se <- sd(boot_Mat$boot_int)
  st.error <- rbind(boot_int_se, boot_slope_se)
  
  # CI of slope
  slope_CI <- mean(boot_Mat$boot_slope) + c(-1, 1) * qt(conf.level / 2, df = n - 1) * boot_slope_se
  names(slope_CI) <- c("conf.low","conf.high")
  head(slope_CI)
  
  # CI for the intercept
  int_CI <- mean(boot_Mat$boot_int) + c(-1, 1) * qt(conf.level / 2, df = n - 1) * boot_int_se
  names(int_CI) <- c("conf.low","conf.high")
  
  # bind them
  CIs <- rbind(int_CI,slope_CI)
  
  # Get the means of the coeffecients
  boot_slope_m <- mean(boot_Mat$boot_slope)
  boot_int_m <- mean(boot_Mat$boot_int)
  estimate <- rbind (boot_int_m, boot_slope_m) # bind them 
  
  # Create data frame of mean coeffecients, se, and CI
  results <- cbind(estimate,st.error,CIs)
  resultd <- data.frame(results)
  colnames(results) <- c("estimate","std.error","conf.low","conf.high")
  rownames(results) <- c("intercept","slope")
  return(results)
}
```

#### Use the function
**Model 1:** log(HomeRange_km2) ~ log(Body_mass_female_mean)
```{r}
lm_sum(d, model = "log(HomeRange_km2) ~ log(Body_mass_female_mean)", conf.level = 0.95)
boot_lm(d, model = "log(HomeRange_km2) ~ log(Body_mass_female_mean)", reps = 1000, conf.level = 0.95)
```
**Model 2:** log(DayLength_km) ~ log(Body_mass_female_mean)
```{r}
lm_sum(d, model = "log(DayLength_km) ~ log(Body_mass_female_mean)", conf.level = 0.95)
boot_lm(d, model = "log(DayLength_km) ~ log(Body_mass_female_mean)", reps = 1000, conf.level = 0.95)
```

**Model 3:**  The last model, log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize. *I have now realized the limits of my boot function....*
```{r}
log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize
lm_sum(d, model = "log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize", conf.level = 0.95)
boot_lm(d, model = "log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize", reps = 1000, conf.level = 0.95)
```
